#output_dir: ./outputs

defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled
  - override hydra/launcher: submitit_slurm

hydra:
  output_subdir: null
  mode: MULTIRUN
  launcher:
    max_num_timeout: 999
    gpus_per_node: 2
    tasks_per_node: ${hydra.launcher.gpus_per_node}
    cpus_per_task: 4
    mem_gb: 48
    timeout_min: 3600
    partition: main
    signal_delay_s: 120
    constraint: lovelace
    # setup:
    #   [
    #     "tar -czf \"$SLURM_TMPDIR/stablewm.tgz\" -C \"$STABLEWM_HOME\" .",
    #     "tar -xzf \"$SLURM_TMPDIR/stablewm.tgz\" -C \"$SLURM_TMPDIR\"",
    #     "rm -f \"$SLURM_TMPDIR/stablewm.tgz\""
    #   ]
  run:
    dir: .

wandb:
  enable: true
  project: xenoworlds
  entity: simon-lacoste-julien

dataset_name: pusht_expert
# cache_dir: null #${oc.env:SLURM_TMPDIR, null}
output_model_name: world_model

trainer:
  max_epochs: 100
  strategy: ddp
  devices: auto
  accelerator: gpu
  precision: 16-mixed

batch_size: 64
num_workers: 8
train_split: 0.9

image_size: 224
patch_size: 16

n_steps: 2
frameskip: 5

dinowm:
  history_size: 3
  num_preds: 1
  proprio_dim: 4
  proprio_embed_dim: 10
  action_dim: 2
  action_embed_dim: 10

predictor:
  depth: 6
  heads: 16
  mlp_dim: 2048
  dim_head: 64
  dropout: 0.1
  emb_dropout: 0.0

predictor_lr: 5e-4
proprio_encoder_lr: 5e-4
action_encoder_lr: 5e-4

dump_object: True
