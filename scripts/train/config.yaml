#output_dir: ./outputs

defaults:
  - _self_
  - override hydra/launcher: submitit_local

# defaults:
#   - _self_
#   # - override hydra/hydra_logging: disabled
#   # - override hydra/job_logging: disabled
#   - override hydra/launcher: submitit_slurm
#
# hydra:
#   output_subdir: null
#   mode: MULTIRUN
#   launcher:
#     max_num_timeout: 999
#     gpus_per_node: 4
#     tasks_per_node: ${hydra.launcher.gpus_per_node}
#     cpus_per_task: 8
#     mem_gb: 64
#     timeout_min: 1080
#     partition: long
#     signal_delay_s: 120
#     constraint: lovelace
#     # setup:
#     # [
#     #   "tar -czf \"$SLURM_TMPDIR/stablewm.tgz\" -C \"$STABLEWM_HOME\" .",
#     #   "tar -xzf \"$SLURM_TMPDIR/stablewm.tgz\" -C \"$SLURM_TMPDIR\"",
#     #   "rm -f \"$SLURM_TMPDIR/stablewm.tgz\""
#     # ]
#   run:
#     dir: .

wandb:
  enable: false
  project: stable-worldmodel
  entity: DanHrmti

dataset_name: pusht_expert_train_video
# cache_dir: null #${oc.env:SLURM_TMPDIR, null}
cache_dir: /users/dharamat/data/dharamat/code/stable-worldmodel/dataset
output_model_name: dinogcbc

trainer:
  max_epochs: 100
  strategy: auto #ddp
  devices: 1
  accelerator: gpu
  precision: 16-mixed

batch_size: 32
num_workers: 16
train_split: 0.9
seed: 42

image_size: 224
patch_size: 16

# n_steps: ${eval:'${dinowm.num_preds} + ${dinowm.history_size}'}
n_steps: 49  # 49 is the minimum episode length in PushT expert dataset. NOTE: -1 gets complete trajectory, only works when all episodes have same length
frameskip: 1

dinowm:
  history_size: 3
  num_preds: 1
  proprio_dim: 4
  proprio_embed_dim: 10
  action_dim: 2
  action_embed_dim: 10

predictor:
  depth: 6
  heads: 16
  mlp_dim: 2048
  dim_head: 64
  dropout: 0.1
  emb_dropout: 0.0

predictor_lr: 5e-4
proprio_encoder_lr: 5e-4
action_encoder_lr: 5e-4

dump_object: True
